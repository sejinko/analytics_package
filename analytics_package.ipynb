{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T13:06:48.293490Z",
     "start_time": "2021-06-15T13:06:46.905302Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 10, 'max_rows', 5, 'max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 셋 만들기\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "## 데이터 전처리\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # 차원축소\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from numpy.linalg import svd # 행렬분해\n",
    "from scipy.sparse.linalg import svds # 행렬분해\n",
    "from scipy.linalg import svd # 행렬분해\n",
    "from sklearn.preprocessing import LabelBinarizer # 원-핫 인코딩 변환 후 희소행렬 최적화 형태로 저장\n",
    "from  scipy.sparse import hstack # 원-핫 인코딩된 희소 행렬을 모두 scipy 패키지의 hstack()함수를 이용하여 결합\n",
    "import gc\n",
    "import nltk\n",
    "import string\n",
    "from nltk import sent_tokenize # 문장 토큰화\n",
    "from nltk import word_tokenize # 단어 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize # 여러 문장들에 대한 단어 토큰화\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams # n-gram\n",
    "import nltk\n",
    "nltk.download('stopwords') # stopwords 제거\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import LatentDirichletAllocation # LDA는 Count기반 Vectorizer만 적용\n",
    "\n",
    "from ast import literal_eval # 텍스트 문자 1차 가공, 파이썬 딕셔너리 변환 후 리스트 형태로 변환\n",
    "\n",
    "## 알고리즘(분류)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "\n",
    "## 알고리즘(군집)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth # 최적의 bandwidth값을 estimate_bandwidth()로 계산 한 뒤에 다시 군집화 수행\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "## 알고리즘(회귀)\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "## 알고리즘(텍스트 마이닝)\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "\n",
    "\n",
    "## 데이터 평가\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score , recall_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "## 기타 모르겠는 알고리즘\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE # 오버샘플링\n",
    "\n",
    "# surprise 패키지\n",
    "from surprise import Reader # load_form_file()을 이용하여 데이터셋 로딩\n",
    "from surprise import SVD\n",
    "from surprise import Dataset \n",
    "from surprise import accuracy \n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import cross_validate # cross_validate()를 이용한 교차 검증\n",
    "from surprise.dataset import DatasetAutoFolds # TrainSet클래스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 로드\n",
    "csv_df = pd.read_csv('./data.csv')\n",
    "\n",
    "# txt 로드(https://rfriend.tistory.com/523)\n",
    "with open('.text.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    text_string = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:59:37.443621Z",
     "start_time": "2021-06-15T12:59:37.419684Z"
    }
   },
   "source": [
    "## 함수로 만들고 들어가서는 그 함수를 적고 다로 적용할 수 있도록 나눠서 만든다.(20분 내로 적어서 만들 수 있게 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# method : 'Standard', 'Minma', 'Log'\n",
    "# l_degree : 2,3,4\n",
    "# input_data : \n",
    "def get_scaled_data(method='None', p_degree=None, input_data=None):\n",
    "    if method == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif method == 'MinMa':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif method == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "        \n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)\n",
    "        \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시나리오를 작성해서 만들어야 함(분류/군집/회귀/)\n",
    "데이터 전처리\n",
    "- 예를 들면 스케일링할때 첨도가 높으면 로그를 씌운다.\n",
    "- null값이 있으면 빼던가 평균으로 대체한다.\n",
    "- 이상치가 있으면 그 데이터를 삭제 후 재 학습/예측/평가\n",
    "- 시계열은 함수로 만들어져 있는게 있으니 그걸 가져다 쓴다고 생각하자\n",
    "\n",
    "staking도 해볼까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform()\n",
    "X_test = scaler.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = RandomForestClassifier(random_state=1)\n",
    "knn = KNeighborsClassifier()\n",
    "log = LogisticRegression()\n",
    "xgb = XGBClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "svc = SVC(probability=True)\n",
    "ext = ExtraTreesClassifier()\n",
    "ada = AdaBoostingClassifier()\n",
    "gnb = GaussianNB()\n",
    "gpc = GaussianProcessClassifier()\n",
    "bag = BaggingClassifier()\n",
    "\n",
    "models = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]\n",
    "model_names = ['Random Forest', 'K Nearest Neighnour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\n",
    "scores = {}\n",
    "               \n",
    "for ind, mod in enumerate(models):\n",
    "    mod.fit(X_train, y_train)\n",
    "    acc = cross_val_score(mod, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    scores[model_names[ind]] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
